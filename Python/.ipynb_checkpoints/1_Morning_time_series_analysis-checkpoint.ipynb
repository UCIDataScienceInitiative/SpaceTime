{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Morning: Introduction to Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this set of exercises we study the basic concepts of statistical time series. We focus on the foundational concepts of *autocorrelation* (https://en.wikipedia.org/wiki/Autocorrelation) and *periodicity* (or *seasonality*) (https://en.wikipedia.org/wiki/Seasonality). Being a statistics workshop, we'll focus on the general quantitative characteristics of time series that require attention to draw proper statistical conclusions from data. This essentially boils down to understanding the consequences of spatial-temporal properties on the calculation of confidence intervals, p.values, posterior probabilities, and other related metrics that quantify statistical evidence. Understanding autocorrelation and periodicity will take us a long way to this goal. \n",
    "\n",
    "First we import the requried packages. Note we also need to load the required R packages using *importr*. But first we configure the Matplotlib backend to show plots inside the notebook before importing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-- Import Required Python Packages\n",
    "import numpy as np\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "#-- Import R packages in Python\n",
    "r = robjects.r\n",
    "nlme = importr('nlme')\n",
    "stats = importr('stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Is there a trend?\n",
    "\n",
    "Here we read in our first real dataset that exhibits typical properties of an environmental time series, and of time series more generally. The measurements are of monthly-averaged sea surface temeprature (SST) for a region of the sea surface enclosed by 5°N–5°S, 120°–170°W, recorded by the National Oceanic and Atmospheric Adminstration (NOAA) and published publically to their website. This particular measurement is used widely as an index of the El Nino phenomena (https://en.wikipedia.org/wiki/El_Ni%C3%B1o) that plays an important role in global climate, and Californian climate in particular. The sea surface measurement represent monthly temperature averages (12 measurements per year) taken by series of measurement techniques including moored buoys and in-situ sampling, averaged according to an assumed measurement error of each technique.  \n",
    "\n",
    "\n",
    "We load the text file as ``sst``, extract our variable of interest ``y`` (sea surface temperature), and define the time variable ``t`` (months). \n",
    "\n",
    "We then go ahead and plot the data with matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sst = np.loadtxt('../Data/detrend.nino34.ascii.txt',skiprows=1) # Get El Nino data from file (included in repository)\n",
    "y = sst[:,2]                                      # Extract SST measurements\n",
    "t1 = np.linspace(1950,2017,len(y))                # Define time axis\n",
    "\n",
    "#-- Plot time-series\n",
    "fig = plt.figure(figsize=(10,6))                  # Set up figure\n",
    "plt.plot(t1,y,'k-')                               # Plot SST as a line\n",
    "plt.ylabel('Sea Surface Temperature',fontsize=15) #label y-axis\n",
    "plt.xlabel('Years',fontsize=15)                   #label x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our first question is perhaps the simplest and most common question asked in applied time series analysis: *Is there a trend over time?* Take a hard look at the plot and write down what you'd guess, yes or no. We'll go back and test our initial intuition after learning a bit of theory.  \n",
    "\n",
    "The simplest statistical trend model is a linear regression with respect to time\n",
    "\n",
    "$$ y_i = \\beta t_i + e_i $$\n",
    "\n",
    "where $y_i$ indicates is a single observation which is a function of the slope with respect to time $\\beta$ multiplied by the $t_i$ variable which represents the time elapsed from the beginning of the time series, while $e_i$ is the background variability not accounted for by the model. We assume this variable has a *Normal* (or *Gaussian*) *distribution*. We often write this statistical model as $e_i \\sim N(0,\\sigma^2)$ where $\\sigma^2$ is the variance of the Normal distribution. It will also be helpful to write the model in vector notation  \n",
    "\n",
    "$$ \\mathbf{y} = \\beta \\mathbf{t} + \\mathbf{e} $$\n",
    "\n",
    "where the bold-faced type indicates the full vector, or column, of values, with $\\mathbf{t}$ representing the set of sequentially increasing time values, and $\\mathbf{e}$ is a *random vector* with *covariance matrix* $\\mathbf{\\Sigma}$. We will learn to love the covariance matrix. It will be the central quantity required to extend basic statistical concepts to encompass spatial-temporal data. \n",
    "\n",
    "## Statistical independence\n",
    "Statistical independence is a central idea in the workshop and is intimately related to concepts of covariance. Many statistical analysis proceed under the assumption of independence but this will often be violated when dealing with spatial-temporal data.  To jog your memory, the covariance between two *random variables* (i.e. a variable decribed by a statistical distribution, opposed to a single value) is \n",
    "\n",
    "$$ \\mathrm{cov}(x,y) = \\mathrm{E}\\left[ (x-\\mathrm{E}[x])(y-\\mathrm{E}[y]) \\right] $$\n",
    "\n",
    "where the $\\mathrm{E}$ is used in statistics to denote a theoretical average with respect to the random variables $x$ and $y$. With a sample of data, we calculate the *empirical* or *sample covariance*\n",
    "\n",
    "$$ \\mathrm{\\hat{cov}}(x,y) = \\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}) $$\n",
    "\n",
    "where the sum is taken over all individual data points $i$, while $\\bar{x}$ and $\\bar{y}$ represent the sample (arithmetic) average of $x$ and $y$, respectively, and $\\hat{}$ notation represents an empirical estimate. Note that the correlation between two variables, $\\mathrm{cor}(x,y)$ is simply the covariance devided by a measure of total variance\n",
    "\n",
    "$$ \\mathrm{cor}(x,y) = \\frac{\\mathrm{cov}(x,y)}{\\sigma_x \\sigma_y}  $$\n",
    "\n",
    "where $\\sigma_x$ and $\\sigma_y$ are the standard deviations for the variables $x$ and $y$, respectively. If the two variables are independent, then $\\mathrm{cov}(x,y) = 0$ theoretically, and $\\mathrm{\\hat{cov}}(x,y) = 0$, on average based on observed realizations of the random variable.\n",
    "\n",
    "In our regression model, the assumption of independence enters via the covariance matrix of the model errors. When a random vector is assumed to have independent elements, it has a covariance with diagonal elements according to the variances of each variable. And further, if we assume that each variable has the same variance then the covariance matrix has the following form\n",
    "\n",
    "$$ \\mathbf{\\Sigma}_{iid} = \\begin{bmatrix}\n",
    "\t  \t\\sigma^2 &        0 &  \\dots & 0 \\\\\n",
    "  \t\t\t   0 & \\sigma^2 &   \\dots & 0 \\\\\n",
    "          \\vdots &   \\vdots &\\ddots & \\vdots  \\\\\n",
    "               0 &        0 &\\dots  & \\sigma^2     \\end{bmatrix} =\n",
    "        \\sigma^2 \\begin{bmatrix}\n",
    "\t  \t       1 &        0 &  \\dots & 0 \\\\\n",
    "  \t\t\t   0 &        1 &  \\dots & 0 \\\\\n",
    "          \\vdots &   \\vdots &\\ddots & \\vdots  \\\\\n",
    "               0 &        0 &\\dots  & 1     \\end{bmatrix} $$\n",
    "               \n",
    "where $\\sigma^2$ is the variance, which is factored on the right as the multiplication of the variance times the correlation matrix. This diagonal (independent) form is the most commonly used form of the Gaussian distribution and is the basis of many statistical models in statistics. \n",
    "\n",
    "In general, the way to intpret this matrix is as a big list of covariances (or correlations) between our random variables (recall in statistics we treat each data point as a random variable, where the paricular observation is the realized value from that random variable). Each $i^{th}$ column is a list of the covariances between datapoint $i$ and all datapoints in the dataset. When data are independent, all covariances are zero and so takes this simple diagonal form. \n",
    "\n",
    "## The likelihood function for independent data\n",
    "The background theory for regression and many other statistical models is the likelihood function. It represents the probability of the observations under the assumption of a particular statistical distribution (https://en.wikipedia.org/wiki/Likelihood_function). We write the likelihood function for an independent random vector\n",
    "\n",
    "$$ p(\\mathbf{y} | \\theta,M, \\sigma^2) = \\prod_{i=1}^n p(y_i | \\theta,M,\\sigma^2) = \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\mathrm{exp} \\left( -\\frac{\\sum_{i=1}^n e_i^2}{2\\sigma^2\t}  \\right) $$\n",
    "\n",
    "This is the function that is maximized when one fits a basic statistical model. For simple models, there are generally analytic expressions which is generally where all those annoying formulas come from in introductory statistics. For more complex models, this function is treated numerically on the computer. And also note this function is central to both classical (or *frequentist*) statistics one typically maximizes this function with respect to the parameters to obtain the maximum likelihood parameter estimates. In Bayesian statistics, one multiplies this function by a specified *prior probability distribution* and integrates to yield *posterior probability distribution*. (See https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Maximum_likelihood_approach to see how the formula for linear regression are derived by maximizing the likelihood function.). Some may recall that linear algebra provides a very compact expression for the parameters that maximize the likelihood of an ordinary least squares regression (https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))\n",
    "\n",
    "$$ \\mathbf{\\hat{\\beta}} = \\mathbf{(X'X)^{-1}X'y}  $$\n",
    "\n",
    "where $\\mathbf{X}$ in this case is a matrix formed with a vector of ones and the variable $\\mathbf{t}$ as its columns. \n",
    "\n",
    "Now we use rpy2 to fit this common regression model with `R` packages in Python. Note first we have to convert each Python object to an R object and pass it along to the R \"global environment\" using `robjects.globalenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robjects.globalenv[\"y\"] = robjects.FloatVector(y)   # Add y to the R environmental as a FloatVector R object\n",
    "robjects.globalenv[\"t1\"] = robjects.FloatVector(t1) # Add y to the R environmental as a FloatVector R object\n",
    "fit = r.lm(\"y ~ t1\")                                # linear fit with model y ~ t1\n",
    "#return summary of fit, including everything after the word \"Resduals\" to avoid the long \"call\" information.\n",
    "print str(r.summary(fit))[str(r.summary(fit)).find(\"Residuals:\"):]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the typical output from a regression that ``R`` and ``Python`` users will be familiar with. It is referred to as the $t-table$ or $ANOVA$ table. While we won't get into what all the values mean, since these quantities are the focus of introductory courses, we will note the *t-statistic* which is formed by taking the ratio of the slope magnitude to the magnitude of its standard error\n",
    "\n",
    "$$ t = \\frac{\\hat{\\beta}}{s.e.(\\hat{\\beta})}   $$\n",
    "\n",
    "which we will use as a rough-and-ready measure of statistical confidence in a particular estimate. We generally will not incorporate $p.value$ interpretations into our analysis, although one should recall that $p.value$ statements are derived from the calculated $t$ statistic. As a rough rule of thumb, smallter $t$ values indicate lower confidence in the presence of an effect. The $t$ statistic calculated above indicates relatively strong evidence for a trend (as indicated by the small p.value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Temporal dependence\n",
    "Now we look at a very simple time series model that violates the assumptions of temporal independence. And as we'll see, it's actually a very useful model to model time series in a variety of situations. This is the *first-order autoregressive model*, or *AR1*. We write this model\n",
    "\n",
    "$$ y_{t+1} = \\phi y_t + e_t  $$\n",
    "\n",
    "where now we are using $y$ to refer to a general time series, here shown at individual time points $t+1$ or $t$, $e_t$ is another general Normal random variable with variance $\\sigma^2$, while $\\phi$ is the *autoregressive coefficient* which can take values anywhere on the interval $(-1,1)$.  \n",
    "\n",
    "The main characteristic of this model we wish to study is its *autocorrelation*. Fundamentally, we'll see that values nearer to one another are more similar than values further away. This concept of correlation as a function of distance (or *lag*) will be a key concept throughout the time series and spatial analysis sections. In general autocorrelation will generate large characteristic flucuations in data that need to be accounted for when drawing statistical conclusions.\n",
    "\n",
    "Before diving too much further in, let's get a sense for independent and non-indendent data via simulation. The code below generates sample realizations to compare the output from independent time series and that generated by our $AR1$ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n  = 100\n",
    "phi = 0.95\n",
    "s = 0.1\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(14,8)) # Set up figure and subplots\n",
    "#-- 3 sample realizations\n",
    "for i in range(3):\n",
    "    ax[0,0].plot(np.array(r.rnorm(n,sd=s)))  # purely random process;\n",
    "    # autoregressive model with autoregressive coefficient 0.95\n",
    "    ax[0,1].plot(np.array(stats.arima_sim(100,model=robjects.ListVector({'ar':phi,'sd':s})))) \n",
    "    ax[0,0].set_ylim([-10,10])            # y range for first plot \n",
    "    ax[0,1].set_ylim([-10,10])            # y range for second plot\n",
    "#-- Same as above but with 100 sample realizations\n",
    "for i in range(100):\n",
    "    ax[1,0].plot(np.array(r.rnorm(n)))  \n",
    "    ax[1,1].plot(np.array(stats.arima_sim(100,model=robjects.ListVector({'ar':phi}))))\n",
    "    ax[1,0].set_ylim([-10,10])\n",
    "    ax[1,1].set_ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise\n",
    "In the cell below, try altering the *ar*, and *s* parameters to see how the parameters alter the characteristic flucuations of the time series. Remember, there is nothing going on in these time series except for random noise and autocorrelation.\n",
    "\n",
    "Try fitting the simple trend above to several of the simulated time series to see how often we find significant trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##  Autocovariance\n",
    "Now that we have seen how autocorrelation parameter alters the behaviour of time series, we are going to extend the concept of covariance to time series. Instread of thinking about covariance as between two different variables $x$ and $y$, like we did above, think of covariance among different time points in a single time series $x$ (here referring to a general $x$ that could, for example, represent $y_i$ or $e_i$ in our trend regression above). In a similar way to how we defined covariance between $x$ and $y$, we can describe autocorrelation as the following\n",
    "\n",
    "$$ \\mathrm{cov}(x_i,x_{i+\\tau}) = \\mathrm{E}\\left[ (x_i-\\mathrm{E}[x])(x_{i+\\tau}-\\mathrm{E}[x]) \\right] $$\n",
    "\n",
    "where $\\tau$ is the distance, or *lag* separating the two elements in the time series. If points in time are independent, then $\\mathrm{cov}(x_i,x_{i+\\tau}) = 0$.\n",
    "\n",
    "\n",
    "## The autocorrelation function\n",
    "And like above, we'll consider covariance *matrices* where a single variance can be separated from the pattern of correlations among the variables. Writing the elements of the correlation matrix with generic values\n",
    "\n",
    "$$ \\mathbf{\\Sigma} = \\sigma^2 \\begin{bmatrix}\n",
    "         \t      1 &     c(x_1,x_2) &     c(x_1,x_3) &          \\dots & c(x_1,x_n) \\\\\n",
    "         c(x_2,x_1) &              1 &     c(x_2,x_3) &         \\ddots & c(x_2,x_n)  \\\\\n",
    "         c(x_3,x_1) &     c(x_3,x_2) &              1 &         \\ddots & \\vdots \\\\\n",
    "             \\vdots &         \\ddots &         \\ddots &         \\ddots & c(x_{n-1},x_n)  \\\\\n",
    "         c(x_n,x_1) &     c(x_n,x_2) &          \\dots & c(x_n,x_{n-1}) & 1     \\end{bmatrix} $$\n",
    "\n",
    "where $c(x_i,x_j)$ is the correlation between variable $x_i$ and $x_j$. What we want is a function that can be used to 'fill in' the values of this correlation matrix, and then we can go ahead and incorporate this matrix into our analyses. Very commonly in natural processes, the autocorrelation function for a dataset will take a form where values closer to another are more similar. This general situation requires an autocorrelation that take *distance* or *lag* between variables as its input\n",
    "\n",
    "$$ c(x_t,x_{t+\\tau}) = \\mathrm{cor}(x_t,x_{t+\\tau}) $$\n",
    "\n",
    "In this case we are taking about lags in time, but we'll see later on the concept generalizes to spatial dimensions as well.\n",
    "\n",
    "## The empirical autocorrelation function\n",
    "Analogous to theoretical and empirical covariances, we can think about either theoretical and empirical autocorrelation functions. The empirical autocorrelation is actually easy to understand. With a sample of data we can form *lagged pairs* with a time series and estimate their correlation. That is, copy the time series and shift the second in time and calculate the resulting correlation between the original and shifted vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(16,10))            # Set up figure and subplots\n",
    "ax[0,0].scatter(y[:-1],y[1:],edgecolor='k',facecolor='w')      # scatter plot of values shifted by 1\n",
    "ax[0,0].set_xlabel('y[0:(n-1)]',fontsize=16)            # x axis label\n",
    "ax[0,0].set_ylabel('y[1:n]',fontsize=16)                # y axis label\n",
    "ax[0,1].scatter(y[:-2],y[2:],edgecolor='k',facecolor='w')      # scatter plot of values shifted by 2\n",
    "ax[0,1].set_xlabel('y[0:(n-2)]',fontsize=16)            # x axis label\n",
    "ax[0,1].set_ylabel('y[2:n]',fontsize=16)                # y axis label\n",
    "ax[1,0].scatter(y[:-3],y[3:],edgecolor='k',facecolor='w')      # scatter plot of values shifted by 3\n",
    "ax[1,0].set_xlabel('y[0:(n-3)]',fontsize=16)            # x axis label\n",
    "ax[1,0].set_ylabel('y[3:n]',fontsize=16)                # y axis label\n",
    "ax[1,1].scatter(y[:-4],y[4:],edgecolor='k',facecolor='w')      # scatter plot of values shifted by 4\n",
    "ax[1,1].set_xlabel('y[0:(n-14]',fontsize=16)            # x axis label\n",
    "ax[1,1].set_ylabel('y[4:n]',fontsize=16)                # y axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In ``R`` we can compute the correlations from all these lagged pairs via the convienient command ``acf()``. In R this would also plot the results. You will get an `R` plot in a separate window. However, in order to make a `matplotlib` plot, in `Python` we would have to extract the results as numpy arrays and plot them, as shown below.  And note that we can standardize relative to the total variance, or not, yielding the *autocorrelation* and *autocovariance* functions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,5))            # Set up figure and subplots\n",
    "acf_corr = np.squeeze(np.array(stats.acf(robjects.FloatVector(y), ci=0, main='', type='correlation')[0]))\n",
    "acf_cov = np.squeeze(np.array(stats.acf(robjects.FloatVector(y), ci=0, main='', type='covariance')[0]))\n",
    "n = len(acf_corr) #number of lags\n",
    "#-0 Plot Autocorrelation\n",
    "ax[0].vlines(range(n),[0],acf_corr)       # plot vertical lines\n",
    "ax[0].plot(range(n),np.zeros(n))          # plot zero line\n",
    "ax[0].set_ylabel('ACF',fontsize=15)       # label y axis  \n",
    "ax[0].set_xlabel('Lag', fontsize=15)      # label x axis\n",
    "#-- Plot Autocovariance\n",
    "ax[1].vlines(range(n),[0],acf_cov)        # plot vertical lines\n",
    "ax[1].plot(range(n),np.zeros(n))          # plot zero line\n",
    "ax[1].set_ylabel('ACF (cov)',fontsize=15) # label y axis\n",
    "ax[1].set_xlabel('Lag',fontsize=15)       # label x axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Autoregressive autocorrelation function\n",
    "It turns out that our first order autoregressive process ($AR1$) has a very nice formula to fill in the values of our correlation matrix\n",
    "\n",
    "$$ c(x_t,x_{t+\\tau}) = \\phi^{\\tau}. $$ \n",
    "\n",
    "which says that the correlation is simply a function of lag between pointsm and by knowing $\\phi$ we can fill in the values of the matrix. This will the autocorrelation function that we will use to analyze time series. It is perhaps the most fundamental and widely used time series model for a variety of purposes. The first order autoregressive covariance matrix is written  \n",
    "\n",
    "$$ \\mathbf{\\Sigma}_{AR1} = \\sigma^2 \\begin{bmatrix}\n",
    "\t  \t       1 &   \\phi & \\phi^2 &  \\dots & \\phi^n \\\\\n",
    "  \t\t  \\phi &        1 &  \\phi &  \\ddots & \\phi^{n-1} \\\\\n",
    "        \\phi^2 &     \\phi &     1 &  \\ddots & \\vdots \\\\\n",
    "        \\vdots &   \\ddots &\\ddots &  \\ddots & \\phi^{n-(n-1)}  \\\\\n",
    "        \\phi^n &\\phi^{n-1}  & \\dots & \\phi^{n-(n-1)} & 1     \\end{bmatrix} $$\n",
    "               \n",
    "To understand where the formula comes from we can observe the following\n",
    "\n",
    "$$ \\bar{y}_{t+\\tau} = \\phi \\bar{y}_{(t+\\tau)-1} = \\phi( \\phi \\bar{y}_{(t+\\tau)-2}) = ,..., = \\phi^n \\bar{y}_{t-n}.$$\n",
    "\n",
    "Now we will try to estimate an autocorrelation function from the time series. First we find the empirical correlation between values lagged by one unit. We then construct the *theoretical autocorrelation function* according to an autoreggressive statistical process. Overtop we plot the *empirical autocorrelation function* using the ``plot_acf`` function in the ``statsmodel`` module (the `Python` version of `acf` in R that we used above, except this function automatically created a `matplotlib` plot), which does the work for you by taking the time series, laging by $\\tau$ units and computing the correlation. In the third panel I used am *optimized* value for the autocorrelation function which came from statistically fitting a curve to the empirical autocorrelation function. We'll learn how to do this below. For now, recognize that the empirical autocorrelation has some noise and while the autoregressive model appears to be a good functional form, we can find a better characterize via a statistical fit. There appears to be some cyclic structure in the correlations. We will get to that later when we address periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(17,5))            # Set up figure and subplots\n",
    "ax[0].scatter(y[:-1],y[1:],color='k',marker='*')      # scatter plot of values shifted by 1\n",
    "ax[0].set_xlabel('y[1:(n-1)]',fontsize=16)            # x axis label\n",
    "ax[0].set_ylabel('y[2:n]',fontsize=16)                # y axis label\n",
    "sm.graphics.tsa.plot_acf(y,lags=30,alpha=np.float('nan'),ax=ax[1]) # plot empirical autocorrelation function\n",
    "alpha_hat = np.corrcoef(y[:-1],y[1:])[0,1]           # the correlation coefficient between lagged values\n",
    "ax[1].plot(np.linspace(0,20),alpha_hat**np.linspace(0,20))        # line for theoretical acf using correlation coeff\n",
    "ax[1].set_xlabel('Lag', fontsize=14)                  # x axis label\n",
    "ax[1].set_ylabel('ACF',fontsize=14)                   # y axis label\n",
    "sm.graphics.tsa.plot_acf(y,lags=30,alpha=np.float('nan'),ax=ax[2]) # plot empirical autocorrelation function\n",
    "ax[2].plot(np.linspace(0,20),0.78**np.linspace(0,20)) # plot optimized autocorrelation function (more info below)\n",
    "ax[2].set_xlabel('Lag', fontsize=14)                  # x axis label\n",
    "ax[2].set_ylabel('ACF',fontsize=14)                   # y axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear regression with autocorrelation\n",
    "To show you how modeling autocorrelation works in practice, we again look back to our El Nino time series regression. From observing the plot of El Nino data it appears that the observed sst $y$ has autocorrelation - i.e. *values closer to one another seem more similar than values far away*. And although we have not visualized $e_i$, we know from the model that $e$ and $y$ only differ via the subtraction of a straight line, so temporal flucuations in $y_i$ will propagate to $e_i$. So in a nutshell, it looks like the assumption of independence in violated.\n",
    "\n",
    "We can write the likelihood function for the correlated data as follows, now incorporating the general covariance matrix\n",
    "\n",
    "$$ p(\\mathbf{y} | \\mathbf{\\theta},M, \\mathbf{\\Sigma}) = \\prod_{i=1}^n p(y_i | \\mathbf{\\theta},M,\\mathbf{\\Sigma}) = \n",
    "\t\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\mathbf{\\Sigma}|^{\\frac{1}{2}}} \n",
    "\t\\mathrm{exp} \\left( -\\frac{1}{2} \\mathbf{e}'\\mathbf{\\Sigma}^{-1}\\mathbf{e}  \\right) $$\n",
    "\n",
    "In the same way we had an expression for maximizing the probability of independent data, there is a formula for the regression parameters in the presents of autocorrelation. This is known as generalized least squares (https://en.wikipedia.org/wiki/Generalized_least_squares).\n",
    "\n",
    "$$ \\mathbf{\\hat{\\beta}} = \\mathbf{(X'\\Sigma^{-1}X)^{-1}X'\\Sigma^{-1}y}  $$\n",
    "\n",
    "To fit this model in ``R`` we will use the ``nlme`` package which is a general set of functions for a wide variety of statistical modeling. Let's go ahead and fit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression between y and time, specifying a first order autoregressive covariance matrix. \n",
    "# Specify method='ML' to fit exact maximum likelihood\n",
    "print r.summary(nlme.gls(r.formula(\"y ~ t1\"), correlation=nlme.corAR1()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So that was easy. Things look very similar to fitting an ordinary least squares regression for independent data. But what did we do? We passed the critical command to ``R`` specifying the correlation structure ``corAR1()`` (also part of `nlme`) which indicates to fit a covariance matrix like the one we looked at above. Note the ``method='ML'`` tells ``R`` to use direct maximum likelihood estimation so we can interpret the objective function value which will be helpful below. You should note one critical thing with this fit. We told ``R`` to specify the correlation *structure* but we did not give it a correlation parameter $\\phi$. But as you see, the output from ``gls()`` returns a value. This is the optimized value yielded by numerically maximizing a likelihood with respect to this parameter. Notably, the t-statistic is considerably smaller than with our assumption of ! Previously we had calculated a $t$ of 4.961 which strongly suggests a statistical trends, while here our $t$ statistic is ~3.5x smaller. Also note that our linear slope is about 25% larger in magnitude here as well. This often happens. But there is an important result from statistics that says the slope in the two cases should not be systematically different (i.e. the estimate is unbiased *on average*), whereas the uncertainty is biased in the sense that ignoring autocorrelation will almost always result in *overconfidence*.  \n",
    "\n",
    "Note that unlike `r.lm()`, the summary of `nlme.gls()` dooes not include extensive \"call\" information so we didn't have to parse the output of `r.summary()` as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Analysis of Orange Country ozone time series\n",
    "Now lets' dig into a higher resolution, multidimentional dataset, analyzing the air quality monitoring data from the Enrivionmental Protection Agency (EPA). These data are readily used for cutting edge research purposes. In fact, you can find a 2017 PNAS paper analyzing these data and published by an Earth System Science graduate student right here: https://goo.gl/eYTS1O. You can find the larger dataset, including weather stations all over the USA and many more measured parameters, available for download from the EPA here: https://goo.gl/QxJv0y. \n",
    "\n",
    "So first we will read in the dataset. For some unknown reason, the EPA website provides data to 2014 with an incomplete 2016 year. To make thing simple we just drop the 2016 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('../Data/ozone_orange.csv')      # Read Ozone data from CSV file\n",
    "ind = np.nonzero(d['year']!=2016)                # Get indices for data before 2016 for simplicity (incomplete chunk of observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple trend analysis\n",
    "Let's try investigating simple linear trends in the ozone data. We're again going to use the ``nlme`` pacakge in ``R`` which optimizes the autoregressive coefficient and accounts for the covariance when calculating uncertainty.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y1 = np.array(d['ozone'])[ind]                   # Extract the daily mean ozone variable\n",
    "years = np.array(d['year'])[ind]                 # Extract years\n",
    "t1 = np.linspace(years.min(),years.max(),len(y1))#define time variable\n",
    "robjects.globalenv[\"y1\"] = robjects.FloatVector(y1) # Add y1 to R envrionment as FloatVector\n",
    "robjects.globalenv[\"t1\"] = robjects.FloatVector(t1) # Add y1 to R envrionment as FloatVector\n",
    "#-- display the summary of a fitted regression with autoregressive covariance matrix\n",
    "print r.summary(nlme.gls(r.formula(\"y1 ~ t1\"), correlation=nlme.corAR1(),method='ML'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Harmonic regression\n",
    "Now we consider another key aspect of time series analysis: *periodicity* or *seasonality*. A yearly seasonal cycle is common in environmental data, but there are often other daily and other periodicities as well. For example, El Nino has a somewhat irregular periodicity, with a mean period of approximately seven years. Other environmental phenomena have different periods, including daily, monthly, multidecadal, as well as longer periods present in geological timescales. Of course we didn't have environmental sensors back then, but geologists use time series analysis of so-called *proxy records* which they analyze to understand much longer periods. \n",
    "\n",
    "In general we  describe periodicities with sines and cosines, of varying complexity depending on the complexity of the periodic pattern. The simplest form is a single frequency decribed by a single term \n",
    "\n",
    "$$ x_t = A\\cos(2\\pi \\omega t + \\zeta) $$\n",
    "\n",
    "$A$ is the amplitude, $\\omega$ is the frequency , and $\\zeta$ is the phase. Although conceptually simple, this term actually poses a bit of a problem for our linear regression framwork since the phase parameter enters the model nonlinearly and some more complex statistics would be required to fit it. Rest assured there is a common fix to this, involving the double angle formula we learned way back when\n",
    "\n",
    "$$ \\cos(a+b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b).  $$\n",
    "\n",
    "This formula convieniently allows us to use two linear terms to model a single period within a linear regression while accounting for the phase. Note the phase no longer enters the model nonlinearly, defining $\\beta_1 = A\\cos(\\zeta)$ and $\\beta_2 = -A\\sin(\\zeta)$\n",
    "\n",
    "Below we fit a seasonal cycle term. We first find the number of years in the time seasons (i.e. number of seasons) ``nyrs`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nyrs= years.max() - years.min()         # number of seasonal cycles [years]\n",
    "f = np.linspace(0,2*np.pi*nyrs,len(y1)) # nyrs cycles over the length of the time series [radians]\n",
    "sinf = np.sin(f)                        # sine wave of f\n",
    "cosf = np.cos(f)                        # cose wave of f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robjects.globalenv[\"sinf\"] = robjects.FloatVector(sinf) # Add sine to R envrionment as FloatVector\n",
    "robjects.globalenv[\"cosf\"] = robjects.FloatVector(cosf) # Add cose to R envrionment as FloatVector\n",
    "#-- fit regression with seasonal component and first order autoregressive covariance matrix\n",
    "print r.summary(nlme.gls(r.formula(\"y1 ~ sinf + cosf\"), correlation=nlme.corAR1(),method='ML'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Time series decomposition\n",
    "So now we have seen how to fit both linear trends and periodic regression terms. The best estimate for the individual terms is when you fit these terms simultaneously. Fitting multiple terms like this is often referred to as *time series decomposition* Below we fit the model simultaneously and then plot the individual terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# store the model object from the fit above\n",
    "fit = nlme.gls(r.formula(\"y1 ~ sinf + cosf + t1\"), correlation=nlme.corAR1(),method='ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k_hat = fit.rx2('coefficients')          # extract fitted regression coefficients\n",
    "print k_hat                              # Display for comparison with R\n",
    "k_hat_var = np.array(fit.rx2('varBeta')) # extract the associated errors\n",
    "print k_hat_var                          # Display for comparison with R\n",
    "phi_hat = stats.coef(fit.rx2(\"modelStruct\"),unconstrained=False) #extract fitted autoregressive parameter\n",
    "e = np.array(fit.rx2('residuals'))       #extract 'raw' residuals (yi - ypred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-- Plot results\n",
    "fig, ax = plt.subplots(2,2,figsize=(16,8))    # set up figure and panels\n",
    "ax[0,0].plot(y1,'k-')                         # plot the data\n",
    "ax[0,0].set_ylim([0,0.07])                    # y axis range\n",
    "ax[0,0].set_ylabel('ozone [ppb]',fontsize=15) # y axis label\n",
    "\n",
    "f_hat = np.mean(y1) + k_hat[1]*sinf + k_hat[2]*cosf  # fitted seasonal cycle\n",
    "ax[0,1].plot(f_hat,'k-')                             # plot fitted seasonal cycle\n",
    "#-- add 95% CIs (upper and lower); note the variance is already std^2\n",
    "ax[0,1].plot(f_hat + 2*np.sqrt(k_hat_var[1,1] + k_hat_var[2,2]),'k:') \n",
    "ax[0,1].plot(f_hat - 2*np.sqrt(k_hat_var[1,1] + k_hat_var[2,2]),'k:')\n",
    "ax[0,1].set_ylim([0,0.07])                           # y axis range\n",
    "\n",
    "t_hat = k_hat[0] + k_hat[3]*t1                       # fitted trend\n",
    "t_hat -= np.mean(t_hat)                              # normalize to the zero-line\n",
    "ax[1,0].plot(t_hat,'k-')                             # plot fitted trend\n",
    "ax[1,0].plot(t_hat + 2*np.sqrt(k_hat_var[3,3]),'k:') # upper 95% CI\n",
    "ax[1,0].plot(t_hat - 2*np.sqrt(k_hat_var[3,3]),'k:') # lower  95% CI\n",
    "ax[1,0].plot(np.zeros(len(t_hat)),'k-')              # add 0 line \n",
    "ax[1,0].set_ylabel('ozone [ppb]',fontsize=15)        # y axis label\n",
    "ax[1,0].set_ylim([-0.001,0.001])                     # y axis range\n",
    "\n",
    "ear = e[1:]-phi_hat*e[:-1]                    # subtract the autoregression\n",
    "ax[1,1].plot(np.mean(y1)+ear,'k-')            # plot the residuals\n",
    "ax[1,1].set_ylim([0,0.07])                    # y axis range\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first three plots should be clear, but the fourth may not be. We took the residuals from model after removing the fitted terms, and then subtracted the estimated autocorrelation function by forming the residuals $e = e^{raw}_{2:n} - \\phi e^{raw}_{1:(n-1)} $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise\n",
    "In the cell below, fit other variables in the ozone dataset. Include a trend and the appropriate seasonal terms. Do any other variables show evidence for a trend after controlling for these effects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Time series regression\n",
    "Just like we fit regression using a time variable, we can fit regressions using other independent variables. We can fit the following model\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x1_i + \\beta_2 x2_i, ..., \\beta_n xn_i + e_i $$\n",
    "\n",
    "these variables can be periodic variables or other variables in the dataset. And like above, we characterize the covariance matrix $\\Sigma$ using the autoregressive correlation function. \n",
    "\n",
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x1 = np.array(d['temp'])[ind]    # Extract temperature\n",
    "x2 = np.array(d['windsp'])[ind]  # Extract wind speed\n",
    "x3 = np.array(d['winddir'])[ind] # Extract wind direction\n",
    "robjects.globalenv[\"x1\"] = robjects.FloatVector(x1)# Add temp to R environment as a FloatVector\n",
    "robjects.globalenv[\"x2\"] = robjects.FloatVector(x2)# Add wind speed to R environment as a FloatVector\n",
    "robjects.globalenv[\"x3\"] = robjects.FloatVector(x3)# Add wind dir to R environment as a FloatVector\n",
    "#-- fit regression with seasonal terms, one input, with autoregressive correlation matrix  \n",
    "fitx1 = nlme.gls(r.formula(\"y1 ~ sinf + cosf + x1\"), correlation=nlme.corAR1(),method='ML')\n",
    "# Fit linear regression with seasonal cycle and 3 independent variables; via maximum likelihood method='ML'\n",
    "fitx123 = nlme.gls(r.formula(\"y1 ~ sinf + cosf + x1 + x2 + x3\"), correlation=nlme.corAR1(),method='ML')\n",
    "df = pd.DataFrame(data={\n",
    "    'model':['fitx1','fitx123'],\n",
    "    'R2':[np.round((np.corrcoef(y1,r.predict(fitx1))[0,1])**2,3)*100,\n",
    "          np.round((np.corrcoef(y1,r.predict(fitx123))[0,1])**2,3)*100],\n",
    "    'BIC':[np.round(np.squeeze(r.summary(fitx1).rx2('BIC')),2),\n",
    "           np.round(np.squeeze(r.summary(fitx123).rx2('BIC')),2)]},\n",
    "                  columns=['model','R2','BIC'])\n",
    "display(df) #display R2 values\n",
    "print r.summary(fitx1)           # Display summary of the fitted model\n",
    "print r.summary(fitx123)         # Display the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Standardize regression variables for interpretation\n",
    "In `R` the `scale()` command to remove variables' repsective means and divide by their respective standard deviations. In Python we can just use the `mean` and `std` atributes of `numpy` arrays to perform this. This is the so-called *z-score transform*. It is very useful in statistics because it nondimentionalizes the data and puts everything on the same scale. Now the slope estimates provide estimates for the relative numerical importance of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x1nrm = (x1-x1.mean())/x1.std()    # scale temp\n",
    "x2nrm = (x2-x2.mean())/x2.std()    # scale wind speed\n",
    "x3nrm = (x3-x3.mean())/x3.std()    # scale wind direction\n",
    "sinfnrm = (sinf-sinf.mean())/sinf.std()\n",
    "cosfnrm = (cosf-cosf.mean())/cosf.std()\n",
    "robjects.globalenv[\"x1nrm\"] = robjects.FloatVector(x1nrm)# Add temp to R environment as a FloatVector\n",
    "robjects.globalenv[\"x2nrm\"] = robjects.FloatVector(x2nrm)# Add wind speed to R environment as a FloatVector\n",
    "robjects.globalenv[\"x3nrm\"] = robjects.FloatVector(x3nrm)# Add wind dir to R environment as a FloatVector\n",
    "robjects.globalenv[\"sinfnrm\"] = robjects.FloatVector(sinfnrm)# Add scaled sine to R environment\n",
    "robjects.globalenv[\"cosfnrm\"] = robjects.FloatVector(cosfnrm)# Add scaled cos to R environment\n",
    "#-- fit regression with seasonal terms, one input, with autoregressive correlation matrix  \n",
    "fitx1nrm = nlme.gls(r.formula(\"y1 ~ sinfnrm + cosfnrm + x1nrm\"),\n",
    "                    correlation=nlme.corAR1(),method='ML')\n",
    "# Fit linear regression with seasonal cycle and 3 independent variables; via maximum likelihood method='ML'\n",
    "fitx123nrm = nlme.gls(r.formula(\"y1 ~ sinfnrm + cosfnrm + x1nrm + x2nrm + x3nrm\"),\n",
    "                      correlation=nlme.corAR1(),method='ML')\n",
    "df = pd.DataFrame(data={\n",
    "    'model':['fitx1nrm','fitx123nrm'],\n",
    "    'R2':[np.round((np.corrcoef(y1,r.predict(fitx1nrm))[0,1])**2,3)*100,\n",
    "          np.round((np.corrcoef(y1,r.predict(fitx123nrm))[0,1])**2,3)*100],\n",
    "    'BIC':[np.round(np.squeeze(r.summary(fitx1nrm).rx2('BIC')),2),\n",
    "           np.round(np.squeeze(r.summary(fitx123nrm).rx2('BIC')),2)]},\n",
    "                  columns=['model','R2','BIC'])\n",
    "display(df) #display R2 values\n",
    "print r.summary(fitx1nrm)           # Display summary of the fitted model\n",
    "print r.summary(fitx123nrm)         # Display the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise\n",
    "Investigate the dataset and try to find a set of variables that appear strongly associated with ozone. Standardize the variables and rank the variables in terms of their slope and their slope t-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Selection\n",
    "When we fit the model with an increasing number of parameters we saw the $R^2$ increase. In fact, $R^2$ always increases with more parameters for linear models. We would therefore like a better metric to determine whether one models statistically fits better than another. There are common metrics used in some cases based on *p.value* calculations, but another more general metric is known as the *Bayesian infromation criterion* (BIC; also known as the Schwartz criterion), and the related *Akakie information criterion* (AIC). Both are based on our likelihood \n",
    "\n",
    "$$ p(\\mathbf{y} | \\mathbf{\\theta},M, \\mathbf{\\Sigma}) = \\prod_{i=1}^n p(y_i | \\mathbf{\\theta},M,\\mathbf{\\Sigma}) = \n",
    "\t\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\mathbf{\\Sigma}|^{\\frac{1}{2}}} \n",
    "\t\\mathrm{exp} \\left( -\\frac{1}{2} \\mathbf{e}'\\mathbf{\\Sigma}^{-1}\\mathbf{e}  \\right) $$\n",
    "\n",
    "where $\\mathbf{\\theta}$ are the parameters of the model, and $\\mathbf{\\Sigma}$ is the covariance matrix of the errors.\n",
    "\n",
    "When we numerically optimize this function to yield *maximum likelihood estimates* we can evaluate the likelihood at the maximum likelihood parameters \n",
    "\n",
    "$$ p(\\mathbf{y} | \\mathbf{\\hat{\\theta}},M, \\mathbf{\\hat{\\Sigma}}) = \\prod_{i=1}^n p(y_i | \\mathbf{\\theta},M,\\mathbf{\\Sigma}) = \n",
    "\t\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\mathbf{\\Sigma}|^{\\frac{1}{2}}} \n",
    "\t\\mathrm{exp} \\left( -\\frac{1}{2} \\mathbf{e}'\\mathbf{\\Sigma}^{-1}\\mathbf{e}  \\right) $$\n",
    "    \n",
    "Note that the quantity $p(\\mathbf{y} | \\mathbf{\\theta},M, \\mathbf{\\Sigma})$ is a general conditional distribution without specified $\\mathbf{\\theta},M, \\mathbf{\\Sigma}$, whereas $p(\\mathbf{y} | \\mathbf{\\hat{\\theta}},M, \\mathbf{\\hat{\\Sigma}})$ is evaluated with specific parameter values and so evaluates to yield a single numerical value. The BIC is then evaluated as a single number\n",
    "\n",
    "$$ \\mathrm{BIC} = -2\\log p(\\mathbf{y} | \\hat{\\mathbf{\\theta}},M,\\hat{\\sigma}^2) + k\\log n $$\n",
    "\n",
    "where $k$ is the number of estimated parameters in the model, and $n$ is the length of the time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dictonary of models (model labels,percent varation explained by the predictions (R2),\n",
    "# Bayesian information criterion (BIC), number of parameters in the models (k))\n",
    "df = pd.DataFrame(data={\n",
    "    'model':['fitx1','fitx123'],\n",
    "    'R2':[(np.corrcoef(y1,r.predict(fitx1))[0,1])**2,(np.corrcoef(y1,r.predict(fitx123))[0,1])**2], \n",
    "    'BIC':[np.squeeze(r.summary(fitx1).rx2('BIC')),np.squeeze(r.summary(fitx123).rx2('BIC'))],\n",
    "    'k':[np.squeeze(r.summary(fitx1).rx2('dims').rx2('p')),np.squeeze(r.summary(fitx123).rx2('dims').rx2('p'))]}, \n",
    "                  columns=['model','R2','k','BIC'])\n",
    "df #Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(16,8))          # Set up figure and panels\n",
    "ax[0,0].plot(y1,'k*')                               # plot data\n",
    "ax[0,0].set_ylabel('y',fontsize=15)                 # y label\n",
    "ax[0,1].plot(r.residuals(fitx1, 'response'),'k*')   # 'raw' residuals\n",
    "ax[0,1].set_ylabel('raw residuals',fontsize=15) \n",
    "ax[1,0].plot(r.residuals(fitx1, 'pearson'),'k*')    # normalized to have unit variance\n",
    "ax[1,0].set_ylabel('standardized residuals',fontsize=15) \n",
    "ax[1,1].plot(r.residuals(fitx1, 'normalized'),'k*') #residuals after removing the autocorrelation\n",
    "ax[1,1].set_ylabel('residuals/COV(i,j)',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the cell below, try to find the single best predictor of mean daily ozone concentration, after correcting for seasonality and autocorrelation. After standardizing variables, fit a few multiple regressions with different input variables rank the models in terms of BIC. Try building a model for other variables beyond ozone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
