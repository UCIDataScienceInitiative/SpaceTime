{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Afternoon: Introduction to Spatial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we've learned the basics of time series autocorrelation, we're ready to extend the concept to spatial dimensions. We'll see that spatial dimensions are a direct generalization of temporal dimension, and the concept of $lag$ carries over almost exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we import the required packages. Again we also have to load the required R packages using `importr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-- Import Required Python Packages\n",
    "import numpy as np\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.spatial\n",
    "import scipy.linalg\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "#-- Import R packages in Python\n",
    "r = robjects.r\n",
    "nlme = importr('nlme')\n",
    "gstat = importr('gstat')\n",
    "sp = importr('sp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you don't already have the `gstat` package for R, you need to install it by running the following cell, which uses the `utils` package to install new packages. Uncomment the commands first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# utils = importr(\"utils\")\n",
    "# utils.install_packages('gstat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then re-run the import cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simulate spatially correlated data\n",
    "To get a sense how autocorrelation creates spatial patterns, let's try simulating data with different properties. Analogous to time series, we'll use a covariance matrix to describe the autocorrelation among variables within a statistical model. And remember again, we treat each observation as arising as a realization from a probability distribution. So when we talk about *correlations in the data* we are really taking about the correlation among these random variables that we describe statistically. In time series we computed correlations as a function of *lag*, but now we will compute correlations as a function of spatial distance.\n",
    "\n",
    "There is an analagous autoregressive model for spatial data; however, it's a little more complicated to numerically optimize so we will use the common *exponential autocorrelation function* which closely approximates the autoregressive model, giving rise to very similar behaviour where *values closer to one another are more typically similar than values further away*. The approximation is $\\alpha^{\\tau} \\approx e^{-r\\tau}$, where $r$ is referred to as the *exponential decay parameter*. The exponential spatial covariance matrix takes the form\n",
    "\n",
    "$$ \\mathbf{\\Sigma}_{exp} = \\sigma^2 \\begin{bmatrix}\n",
    "   \t  \t    1 & e^{-rd_{1,2}} &          e^{-rd_{1,3}} &  \\dots  & e^{-rd_{1,n}} \\\\\n",
    "e^{-rd_{2,1}} &             1 &          e^{-rd_{2,3}} &  \\ddots & e^{-rd_{2,n}} \\\\ \n",
    "e^{-rd_{3,1}} & e^{-rd_{3,2}} &                      1 & \\ddots  & \\vdots \\\\\n",
    "       \\vdots &        \\ddots &                 \\ddots &  \\ddots & e^{-rd_{n-1,n}} \\\\\n",
    "e^{-rd_{n,1}} & e^{-rd_{n,2}} & \\dots & e^{rd_{n,n-1}} &       1 \\end{bmatrix}  $$\n",
    "\n",
    "Below we're going to simulate some data with exponential spatial correlation to get a sense of how this model generates spatial patterns. We'll consider a simulated 30x30 grid of observations. The key function below is the ``dist()`` command which computes the pairwise distances for all synthetic points. The $\\mathbf{C}$ matrix defines the correlation matrix and $\\mathbf{S}$ the resulting covariance matrix. Matrix $\\mathbf{G}$ is the reflection of the covariance which is known as the *variogram*. We'll generally use the variogram for spatial analysis because it turns out to be a bit more convienient in a variety of settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# two column matrix with all pair-wise coordinates\n",
    "M = np.array(np.meshgrid(range(1,31), range(1,31))).reshape(2, 30**2).T \n",
    "n = len(M)                                              # number of pairs\n",
    "D_upper_triangle = scipy.spatial.distance.pdist(M)      # Upper triangle distance matrix\n",
    "D = scipy.spatial.distance.squareform(D_upper_triangle) # convert to square form distance matrix\n",
    "r0 = 0.1                                                # decorrelation parameter\n",
    "s = 10                                                  # variance\n",
    "C = np.exp(-r0*D)                                       # Correlation matrix\n",
    "S = s**2*C                                              # construct covariance matrix based on distance\n",
    "G = s**2*(1 - np.exp(-r0*D))                            # Analogous variogram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(17,4.5))        # Set up figure and subplots\n",
    "dist_fig = ax[0].imshow(S,cmap='jet',aspect='auto') # visualize the covariance matrix\n",
    "fig.colorbar(dist_fig,ax=ax[0])                     # Add colorbar\n",
    "ax[0].set_title('Covariance Matrix')                # Add title\n",
    "corr_fig = ax[1].imshow(G,cmap='jet',aspect='auto') # visualize the variogram matrix\n",
    "fig.colorbar(corr_fig,ax=ax[1])                     # Add colorbar\n",
    "ax[1].set_title('Variogram Matrix')                 # Add title\n",
    "ax[2].hist(D.flatten(),20,edgecolor='k',facecolor='none',linewidth=1.5) # make a histogram of the pair-wise distances\n",
    "ax[2].set_ylabel('Frequency',fontsize=15)           # add y-label\n",
    "ax[2].set_title('Distribution of Distances')        # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The two plots on the left and in the middle visualize the covariance matrix and variogram matrix, respectively. The plot on the right is a frequency histogram of all the pairwise distances. Notice there are the most pairs are of intermediate distance, with fewer points at short and long distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "L = scipy.linalg.cho_factor(S,lower=True)               # Cholesky factorization of the covariance matrix\n",
    "Li = np.linalg.inv(L[0])                                # Inverse of the triangular matrix for back-transform\n",
    "x0 = np.random.normal(size=(1,n)) # random 1xn matrix drawn from Gaussian distribution\n",
    "x = np.dot(x0,L[0])     #transform the random data by the factored covariance matrix to generate spatial correlation\n",
    "x00 = np.dot(x,Li)      #transform back to random data by multiplying by the inverse of the factored covariance matrix\n",
    "#-- plot\n",
    "fig, ax = plt.subplots(1,3,figsize=(15,4))     # Set up figure and subplots\n",
    "x0_fig = ax[0].imshow(x0.reshape(30,30),cmap='jet')  # Show original random matrix\n",
    "fig.colorbar(x0_fig,ax=ax[0])                  # Add colorbar\n",
    "x_fig = ax[1].imshow(x.reshape(30,30),cmap='jet')    # Show transformed random data with spatial correlation\n",
    "fig.colorbar(x_fig,ax=ax[1])                   # Add colorbar\n",
    "x00_fig = ax[2].imshow(x00.reshape(30,30),cmap='jet')# Show correlated data transformed back to random data\n",
    "fig.colorbar(x00_fig,ax=ax[2])                 # Add colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see what's going on, consider the independent vector $\\mathbf{x}_0$ with each element having variance $\\sigma^2$. Recall the covariance matrix for this vector is $\\mathbf{\\Sigma} = \\sigma^2 \\mathbf{I}$. The correlated vector, on the other hand, $\\mathbf{x} = \\mathbf{Lx}_0$ has covariance $\\mathbf{\\Sigma} = S$. To see this, observe\n",
    "\n",
    "$$ \\mathrm{E}(\\mathbf{xx'}) = \\mathrm{E}((\\mathbf{Lx}_0)(\\mathbf{Lx}_0)') = \\mathrm{E}(\\mathbf{Lx}_0\\mathbf{x}_0'\\mathbf{L}') = \\mathbf{L}\\mathrm{E}(\\mathbf{x}_0\\mathbf{x}_0')\\mathbf{L}'$$\n",
    "\n",
    "Now we know from our assumptions that $\\mathrm{E}(\\mathbf{x}_0\\mathbf{x}_0') = \\mathbf{I}$ \n",
    "\n",
    "$$ L\\mathrm{E}(\\mathbf{x}_0\\mathbf{x}_0')\\mathbf{L}' = \\mathbf{L}\\sigma^2\\mathbf{IL}' = \\sigma^2\\mathbf{LL}' = \\mathbf{S} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spatial Interpolation\n",
    "The term *interpolation* refers to the statistical process of 'filling in' values for unobserved locations. It is a very common statistical procedure (https://en.wikipedia.org/wiki/Multivariate_interpolation). There are many models to perform this task, but a common model with good statistical properties is known as Krigeing (https://en.wikipedia.org/wiki/Kriging). For some silly historical reason, people have altered the spelling of the term to kriging. But it is named after Krige, so I think the only spelling that makes any sense is Krigeing.  \n",
    "\n",
    "First we will plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('../Data/oregon_temp_precip.csv')            # Read Oregon data from repository\n",
    "plt.scatter(d['lon'],d['lat'],c=d['temp_annual'],cmap='jet') # scatter plot, colors based on annual temp\n",
    "plt.ylabel(r'latitude [$^o$]',fontsize=15)   # y label\n",
    "plt.xlabel(r'longitude [$^o$]',fontsize=15)  # x label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use properties of the variogram (i.e. reflected variance) to predict values at unobserved locations. The general variogram can be written\n",
    "\n",
    "$$ \\gamma(y_i,y_j) = \\gamma(d=d(y_i,y_j)) $$\n",
    "\n",
    "where the variance between two spatial points (sometimes called *semivariance*) is a function of distance separating them. Our exponential variogram is\n",
    "\n",
    "$$ \\gamma(y_i-y_j) = \\gamma(d) = \\gamma_0 \\left( 1-e^{-rd} \\right)  $$\n",
    "\n",
    "where the distance is defined as as the length of the vector separating $y_i$ and $y_j$. In this workshop we will be using simple Euclidean distance, $ d_E = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2} $, and will also use a package that computes distances according to from a geodesic projection of latitude and longitude onto the surface of the earth. \n",
    "\n",
    "Interpolation is a bit trickier than it seems. Krigeing, or so-called *optimal interpolation* is statistically derived by finding the linear predictor that a theoretical variance. We write the linear interpolation model\n",
    "\n",
    "$$ y_0 = \\sum_{i,j} w_{i,j} y_{i,j}    $$\n",
    "\n",
    "Given the function $\\gamma(d)$, where $d$ is the distance between we can define Krigeing equations. \n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "\t  \\gamma(y_1,y_1) & \\gamma(y_1,y_2) &  \\dots & \\gamma(y_1,y_n) & 1 \\\\\n",
    "   \t  \\gamma(y_2,y_1) & \\gamma(y_2,y_2) &  \\dots & \\gamma(y_2,y_n) & 1 \\\\\n",
    "   \t           \\vdots &          \\vdots & \\ddots &         \\vdots  & 1 \\\\ \n",
    "      \\gamma(y_n,y_1) & \\gamma(y_n,y_2) &  \\dots & \\gamma(y_n,y_n) & 1 \\\\\n",
    "                    1 &               1 &  \\dots &               1 & 0  \\end{bmatrix}\n",
    "\\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ 1 \\end{bmatrix} \n",
    "= \\begin{bmatrix} \\gamma(y_1,y_0) \\\\ \\gamma(y_2,y_0) \\\\ \\vdots \\\\ \\gamma(y_n,y_0) \\\\ 1 \\end{bmatrix} $$\n",
    "\n",
    "This system can be written in the following way, as a block matrix\n",
    "\n",
    "$$ \\left[\n",
    "\\begin{array}{c|c}\n",
    "\\mathbf{\\Gamma} & \\mathbf{1} \\\\ \\hline \\mathbf{1}^T & 0 \\end{array} \\right] \\left[ \\begin{array}{c}  \\mathbf{w} \\\\ \\hline \\bar{y} \\end{array} \\right] = \\left[ \\begin{array}{c} \\mathbf{\\gamma_0} \\\\ \\hline 1 \\end{array} \\right] $$\n",
    "\n",
    "from which the we obtain the least squares estimates of the weights. It can be shown via the *Gauss-Markov theorem* (https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) that these values are the best estimates in terms of being unbiased and having the minimum variance of all possible estimators. This is often referred to in statistics with the acronym BLUE: *best linear unbiased estimator*. The least squares estimates of the weights, given $\\gamma(d)$ is\n",
    "\n",
    "$$ \\mathbf{\\hat{w}} = \\mathbf{\\Gamma^{-1}} \\left[ \\mathbf{\\Gamma_0} - \\bar{y} \\mathbf{1} \\right] $$ \n",
    "\n",
    "Given the estimated weights, we compute the prediction $y_0 = \\sum_{i,j} w_{i,j} y_{i,j}$. The variance of the prediction is given by the expression. Naturally, the prediction variance increases with distance away from the observation via the variogram \n",
    "\n",
    "$$ \\mathrm{Var}(y*) = \\sigma^2 - \\mathbf{w^T}\\mathbf{\\Gamma_0}. $$\n",
    "\n",
    "Now all that is well and good. But the above assumes we know $\\gamma(d)$. In practice this is not the case and we must estimate this via optimization. Turns out this is a bit of a numerical maneuver so we use software. One can estimate the variogram is various ways, but a common and intuitive approach is to fit functional forms to the empirical variogram. This is exactly analogous to the way we fit functional forms to the autocorrelation function. \n",
    "\n",
    "Below we use the ``gstat`` package to compute the empirical variogram. The package is saving us a lot of work here. It is binning the observations according to distance, computing the variance\n",
    "\n",
    "$$ \\hat{\\gamma}(d) = \\frac{1}{2N_d} \\sum_{i,j} (y_i - y_j)^2$$\n",
    "\n",
    "Over top we plot $N_d$ to remind you that there are an unequal number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robjects.globalenv[\"lat\"] = robjects.FloatVector(d['lat'])  # Add lat to R envrionment\n",
    "robjects.globalenv[\"lon\"] = robjects.FloatVector(d['lon'])  # Add lon to R envrionment\n",
    "dsp = pandas2ri.py2ri(d)                                    # Convert d to R DataFrame\n",
    "proj4string = robjects.r['proj4string']                     # get proj4string function\n",
    "SpatialPoints = robjects.r['SpatialPoints']                 # get SpatialPoints function        \n",
    "maptools_set = getattr(sp, 'coordinates<-')      # Get 'coordinates<-' command from sp\n",
    "dsp = maptools_set(dsp,r.formula(' ~ lon + lat'))# Define coordinates; changes dsp to class spatial data frame\n",
    "key =  sp.CRS(\"+proj=longlat +datum=WGS84\")      # get projection key\n",
    "dsp = SpatialPoints(dsp, proj4string = key)      # project latitude/longitude according to equal area\n",
    "\n",
    "robjects.globalenv[\"temp_jan\"] = robjects.FloatVector(d['temp_jan'])\n",
    "d_evg = gstat.variogram(r.formula(\"temp_jan ~ 1\"), dsp, cutoff=500)                  #estimate empirical variogram\n",
    "fig, ax1 = plt.subplots()  # set up figure and axes\n",
    "ax1.scatter(np.array(d_evg.rx2('dist')), np.array(d_evg.rx2('gamma')),label='nobs')  #plot the empirical variogram\n",
    "ax2 = ax1.twinx() # make y axis on the right side\n",
    "#--  plot the number of observations used to compute empirical variances\n",
    "ax2.plot(np.array(d_evg.rx2('dist')),np.array(d_evg.rx2('np')))  \n",
    "\n",
    "ax1.set_xlabel('distance [km]',fontsize=15)\n",
    "ax1.set_ylabel('variance',fontsize=15)\n",
    "ax2.set_ylabel('# of observations',fontsize=15)\n",
    "ax1.legend(loc=2,fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note about using maptools_set: In R, `coordinates(dsp)=~lon+lat` is the same as `dsp <- \"coordinates<-\"(dsp, formula(\"~lon+lat\"))`. But python doesn't have this sort of embedded assignment, so we define the `\"coordinates<-\"` command and pass the object and the formula as arguments in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now our job is to fit a functional form to this empirical scatterplot. In R package ``gstat``, the function ``fit_variogram`` will do this for us, with some fancy backend magic regarding how to treat the unequal number of observations. The function takes the initial values for the variogram parameters as inputs, which tells the function which parameteres to include in the model. Additional arguements can be used to tell the package which parameters to keep fixed. We eyeball and fix the range as it appears to considerably help convergence for this particular dataset. Then we plot the empirical and optimized variogram simultaneously. Note in `Python` we have to first evaluate the $(x,y)$ coordinates of the optimized variogram from the functional form using `gstat.variogramLine()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d_vg = gstat.fit_variogram(d_evg, model=gstat.vgm(psill=20,    #initial sill value for optimizer\n",
    "                           model=\"Exp\",                        #functional form\n",
    "                           range=300,                          #initial range value\n",
    "                           nugget=0.2*np.mean(d['temp_jan'])), #initial nuggest value\n",
    "                           fit_ranges=False)                   #tell the package not to optimize range (helps with convergence)\n",
    "\n",
    "vg_points = gstat.variogramLine(d_vg,np.max(np.array(d_evg.rx2('dist')))) # fill out values from variogram model\n",
    "plt.scatter(np.array(d_evg.rx2('dist')), np.array(d_evg.rx2('gamma')))    #plot the empirical variogram\n",
    "plt.plot(np.array(vg_points.rx2('dist')),np.array(vg_points.rx2('gamma')))# plot optimized variogram  \n",
    "plt.xlabel('distance [km]',fontsize=15)\n",
    "plt.ylabel('variance',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As pointed out above, the goal of Krigeing is often to interpolate values at unobserved locations. Given the optimized variogram, we can now construct our necessary function $\\gamma(d)$ for the observations and new point to predict from. Similarly, we can construct the variance of those predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_lat, max_lat = np.min(d['lat']), np.max(d['lat']) #obserbed latitude range\n",
    "min_lon, max_lon = np.min(d['lon']), np.max(d['lon']) #observed longitude range\n",
    "\n",
    "res = 0.05                                            #resolution of input lat/lon grid [degrees]\n",
    "lat0 = np.arange(min_lat,max_lat,res)                 #input latitudes\n",
    "lon0 = np.arange(min_lon,max_lon,res)                 #input longitudes\n",
    "expand_grid = robjects.r['expand.grid']               #get expand.grid function from R\n",
    "grd = expand_grid(lon=lon0,lat=lat0)                  #input grid  \n",
    "\n",
    "maptools_set = getattr(sp, 'coordinates<-')      # Get 'coordinates<-' command from sp\n",
    "grd = maptools_set(grd,r.formula(' ~ lon + lat'))# Define coordinates; changes dsp to class spatial data frame\n",
    "key =  sp.CRS(\"+proj=longlat +datum=WGS84\")      # get projection key\n",
    "grd = SpatialPoints(grd, proj4string = key)      # project latitude/longitude according to equal area\n",
    "gridded_func = getattr(sp, 'gridded<-')          # Get 'Gridded<-' command from sp\n",
    "grd = gridded_func(grd,True)\n",
    "\n",
    "#-- krigeing prediction; temp_jan ~ 1 specifies the mean, locations defines \n",
    "#-- observation points, newdata represents prediction points\n",
    "d_k = gstat.krige(formula=r.formula(\"temp_jan ~ 1\"), locations=dsp, newdata=grd, model=d_vg)\n",
    "dk_data = np.array(d_k.slots['data']).T         # Extracted interpolated data and take transpose to match coord shape\n",
    "dk_coords = np.array(d_k.slots['coords'])       # Extract coordinates\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,6))      # Set up figure and subplots\n",
    "#-- plot interpolated field\n",
    "int_fig = ax[0].scatter(dk_coords[:,0],dk_coords[:,1],c=dk_data[:,0],cmap='jet')\n",
    "fig.colorbar(int_fig,ax=ax[0])                    # Add colorbar\n",
    "ax[0].scatter(d['lon'],d['lat'],s=50,facecolors='none', edgecolors='white',linewidth=1.5) #add original data points\n",
    "ax[0].set_xlabel(r'Longitude [$^o$]',fontsize=15) # Add x axis label\n",
    "ax[0].set_ylabel(r'Latitude [$^o$]',fontsize=15)  # Add y axis label\n",
    "ax[0].set_title('Interpolated Field',fontsize=14)\n",
    "#-- plot variance\n",
    "var_fig = ax[1].scatter(dk_coords[:,0],dk_coords[:,1],c=dk_data[:,1],cmap='jet')\n",
    "fig.colorbar(var_fig,ax=ax[1])                    # Add colorbar\n",
    "ax[1].scatter(d['lon'],d['lat'],s=50,facecolors='none', edgecolors='white',linewidth=1.5) #add original data points\n",
    "ax[1].set_xlabel(r'Longitude [$^o$]',fontsize=15) # Add x axis label\n",
    "ax[1].set_ylabel(r'Latitude [$^o$]',fontsize=15)  # Add y axis label\n",
    "ax[1].set_title('Variance',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spatial regression\n",
    "Again following from the time series regression, we can investigate the relation among two variables while accounting for spatial autocorrelation. The linear regression we will fit is written as \n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x1_i + \\beta_2 x2_i + ,..., + \\beta_n xn_i + e_i  $$\n",
    "\n",
    "with a number of input varibles $n$, where $e_i$ is a Gaussian variable with spatial covariance $\\mathbf{\\Sigma}$. \n",
    "\n",
    "We take a look at the first few rows of the data to remind us of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d.head(n=3)     #display the top three rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below we fit the regression model with a single covariate and an exponential covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robjects.globalenv[\"precip_ann\"] = robjects.FloatVector(d['precip_ann'])  # Add precip to R envrionment \n",
    "robjects.globalenv[\"temp_annual\"] = robjects.FloatVector(d['temp_annual'])# Add precip to R envrionment \n",
    "robjects.globalenv[\"lat\"] = robjects.FloatVector(d['lat'])                # Add lat to R envrionment\n",
    "robjects.globalenv[\"lon\"] = robjects.FloatVector(d['lon'])                # Add lon to R envrionment\n",
    "#-- fit linear regression model while account for spatial autocorrelation \n",
    "fit = nlme.gls(r.formula(\"precip_ann ~ temp_annual\"),\n",
    "               correlation=nlme.corExp(form=r.formula(\"~lat+lon\"),nugget=False),method='ML')  \n",
    "print r.summary(fit)    # print a summary of the fitted model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robjects.globalenv[\"elevation\"] = robjects.FloatVector(d['elevation'])  # Add eleavtion to R envrionment\n",
    "#-- fit regression model to two parameters\n",
    "fit2 = nlme.gls(r.formula(\"precip_ann ~ temp_annual + elevation\"), \n",
    "                correlation=nlme.corExp(form=r.formula(\"~lat+lon\"),nugget=False),method='ML')   \n",
    "print r.summary(fit2)     # summary of the fitted model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just like we fitted a variogram to observed data, we can also fit a variogram model to model predictions. Below we take the predictions from the linear regression model and estimate their predicted empirical variogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robjects.globalenv[\"pred\"] = robjects.FloatVector(r.predict(fit2))  # Add prediction to R envrionment \n",
    "pred_evg = gstat.variogram(r.formula(\"pred ~ 1\"), dsp, cutoff=500)  # variogram\n",
    "#plot the empirical variogram\n",
    "plt.plot(np.array(pred_evg.rx2('dist')), np.array(pred_evg.rx2('gamma')),'ko')  \n",
    "plt.xlabel('distance [km]',fontsize=15)\n",
    "plt.ylabel('variance',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_lat, max_lat = np.min(d['lat']), np.max(d['lat']) #obserbed latitude range\n",
    "min_lon, max_lon = np.min(d['lon']), np.max(d['lon']) #observed longitude range\n",
    "\n",
    "res = 0.05                                            #resolution of input lat/lon grid [degrees]\n",
    "lat0 = np.arange(min_lat,max_lat,res)                 #input latitudes\n",
    "lon0 = np.arange(min_lon,max_lon,res)                 #input longitudes\n",
    "expand_grid = robjects.r['expand.grid']               #get expand.grid function from R\n",
    "grd = expand_grid(lon=lon0,lat=lat0)                  #input grid  \n",
    "\n",
    "maptools_set = getattr(sp, 'coordinates<-')      # Get 'coordinates<-' command from sp\n",
    "grd = maptools_set(grd,r.formula(' ~ lon + lat'))# Define coordinates; changes dsp to class spatial data frame\n",
    "key =  sp.CRS(\"+proj=longlat +datum=WGS84\")      # get projection key\n",
    "grd = SpatialPoints(grd, proj4string = key)      # project latitude/longitude according to equal area\n",
    "gridded_func = getattr(sp, 'gridded<-')          # Get 'Gridded<-' command from sp\n",
    "grd = gridded_func(grd,True)\n",
    "\n",
    "pred_vg = gstat.fit_variogram(pred_evg, model=gstat.vgm(psill=20,model=\"Exp\",range=200,\n",
    "                                                   nugget=0.2*np.mean(d['temp_jan'])), fit_ranges=False)\n",
    "\n",
    "#-- krigeing prediction; temp_jan ~ 1 specifies the mean, locations defines \n",
    "#-- observation points, newdata represents prediction points\n",
    "d_k = gstat.krige(formula=r.formula(\"pred ~ 1\"), locations=dsp, newdata=grd, model=pred_vg)\n",
    "dk_data = np.array(d_k.slots['data']).T         # Extracted interpolated data and take transpose to match coord shape\n",
    "dk_coords = np.array(d_k.slots['coords'])       # Extract coordinates\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,6))      # Set up figure and subplots\n",
    "#-- plot interpolated field\n",
    "int_fig = ax[0].scatter(dk_coords[:,0],dk_coords[:,1],c=dk_data[:,0],cmap='jet')\n",
    "fig.colorbar(int_fig,ax=ax[0])                    # Add colorbar\n",
    "ax[0].scatter(d['lon'],d['lat'],s=50,facecolors='none', edgecolors='white',linewidth=1.5) #add original data points\n",
    "ax[0].set_xlabel(r'Longitude [$^o$]',fontsize=15) # Add x axis label\n",
    "ax[0].set_ylabel(r'Latitude [$^o$]',fontsize=15)  # Add y axis label\n",
    "ax[0].set_title('Interpolated Field',fontsize=14)\n",
    "#-- plot variance\n",
    "var_fig = ax[1].scatter(dk_coords[:,0],dk_coords[:,1],c=dk_data[:,1],cmap='jet')\n",
    "fig.colorbar(var_fig,ax=ax[1])                    # Add colorbar\n",
    "ax[1].scatter(d['lon'],d['lat'],s=50,facecolors='none', edgecolors='white',linewidth=1.5) #add original data points\n",
    "ax[1].set_xlabel(r'Longitude [$^o$]',fontsize=15) # Add x axis label\n",
    "ax[1].set_ylabel(r'Latitude [$^o$]',fontsize=15)  # Add y axis label\n",
    "ax[1].set_title('Variance',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alternative autocorrelation functions\n",
    "So far we have considered two closely related autocorrelation functions, the *first order autoregressive* covariance for time series, and the *exponential* autocovariance for spatial data. There is actually many functional forms we could use. Again casting the autocorrelation function in terms of the variogram, we can take a look at the available variograms in ``gstat`` by issueing the command  ``show.vgms()``. Note the plot should appear in a separate window (If an empty window pops up, run the cell again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p = gstat.show_vgms()  \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below we optimize the parameters for two different functional forms and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_vg1 = gstat.fit_variogram(pred_evg, model=gstat.vgm(psill=20,model=\"Exp\",range=200,\n",
    "                                                   nugget=0.2*np.mean(d['temp_jan'])), fit_ranges=False)\n",
    "pred_vg2 = gstat.fit_variogram(pred_evg, model=gstat.vgm(psill=20,model=\"Sph\",range=200,\n",
    "                                                   nugget=0.2*np.mean(d['temp_jan'])), fit_ranges=False)\n",
    "vg_points = {} # make a dictionary for the optimized variogram values for both models \n",
    "vg_points[0] = gstat.variogramLine(pred_vg1,\n",
    "                                 np.max(np.array(pred_evg.rx2('dist')))) # fill out values from variogram model\n",
    "vg_points[1] = gstat.variogramLine(pred_vg2,\n",
    "                                 np.max(np.array(pred_evg.rx2('dist')))) # fill out values from variogram model\n",
    "fig,ax = plt.subplots(2,1,figsize=(6,12))\n",
    "for i in [0,1]: # loop over the subplots and plot both variograms\n",
    "    ax[i].scatter(np.array(pred_evg.rx2('dist')), np.array(pred_evg.rx2('gamma')))     #plot empirical variogram\n",
    "    ax[i].plot(np.array(vg_points[i].rx2('dist')),np.array(vg_points[i].rx2('gamma'))) # plot optimized variogram\n",
    "    ax[i].set_xlabel('distance [km]',fontsize=15)    # set x label \n",
    "    ax[i].set_ylabel('variance',fontsize=15)         # set y label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we learned in the time series section, we can also perform model selection with respect to spatial regression variables and also spatial variograms. We again use the Bayesian information criterion (BIC) as a simple metric to balance the quality of the fit and the number of parameters used to fit the model\n",
    "\n",
    "$$ \\mathrm{BIC} = -2\\log p(\\mathbf{y} | \\hat{\\mathbf{\\theta}},M,\\hat{\\sigma}^2) + k\\log n $$\n",
    "\n",
    "where $\\log p(\\mathbf{y} | \\hat{\\mathbf{\\theta}},M,\\hat{\\sigma}^2)$ is the log likelihood for the observations evaluated at the optimal parameters, conditional on the model $M$, $k$ is the number of free parameters used to fit the model, and $n$ is the number of data points in the fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-- fit the regression with exponential covariance matrix\n",
    "fit1 = nlme.gls(r.formula(\"precip_ann ~ temp_annual\"),\n",
    "                correlation=nlme.corExp(form=r.formula(\"~lat+lon\"),nugget=False),method='ML')\n",
    "fit2 = nlme.gls(r.formula(\"precip_ann ~ elevation\"),\n",
    "                correlation=nlme.corExp(form=r.formula(\"~lat+lon\"),nugget=False),method='ML')\n",
    "fit3 = nlme.gls(r.formula(\"precip_ann ~ temp_annual + elevation\"),\n",
    "                correlation=nlme.corExp(form=r.formula(\"~lat+lon\"),nugget=False),method='ML')\n",
    "#-- display the Bayesian Information Criterion of the fits     \n",
    "bic = r.BIC(fit1,fit2,fit3)\n",
    "bic_dic = dict(zip(bic.names, map(list,list(bic)))) # convert to python dictionary\n",
    "bic_dic['models'] = ['fit1','fit2','fit3']          # Add model info\n",
    "bic_pd = pd.DataFrame(data=bic_dic,columns=np.concatenate([['models'],bic.names])) #make into Pandas DataFrame\n",
    "bic_pd # Display BIC table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-- fit the regression with exponential covariance matrix\n",
    "fit1spher = nlme.gls(r.formula(\"precip_ann ~ temp_annual\"),\n",
    "                     correlation=nlme.corSpher(form=r.formula(\"~lat+lon\"),nugget=False),method='ML') \n",
    "fit1exp = nlme.gls(r.formula(\"precip_ann ~ temp_annual\"),\n",
    "                   correlation=nlme.corExp(form=r.formula(\"~lat+lon\"),nugget=False),method='ML')\n",
    "\n",
    "#-- display the Bayesian Information Criterion of the fits     \n",
    "bic = r.BIC(fit1spher,fit1exp)\n",
    "bic_dic = dict(zip(bic.names, map(list,list(bic)))) # convert to python dictionary\n",
    "bic_dic['models'] = ['fit1spher','fit1exp']         # Add model info\n",
    "bic_pd = pd.DataFrame(data=bic_dic,columns=np.concatenate([['models'],bic.names])) #make into Pandas DataFrame\n",
    "bic_pd # Display BIC table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
